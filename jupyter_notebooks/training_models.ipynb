{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "elXqF2pGdTow",
        "colab_type": "code",
        "outputId": "5c1c714d-c4bd-47f8-bd49-7733f73a7244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIisEPTe1mt2",
        "colab_type": "code",
        "outputId": "dd675f7b-b544-410a-f03c-65f102c7734c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "import pathlib\n",
        "import time\n",
        "import os\n",
        "\n",
        "import torch\n",
        "!pip install pytorch_pretrained_bert==0.4.0\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from pytorch_pretrained_bert.modeling import PreTrainedBertModel\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
        "\n",
        "from torch.utils import data\n",
        "from torch import nn, optim\n",
        "from torch.nn.functional import relu, sigmoid\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/68/84de54aea460eb5b2e90bf47a429aacc1ce97ff052ec40874ea38ae2331d/pytorch_pretrained_bert-0.4.0-py3-none-any.whl (45kB)\n",
            "\r\u001b[K     |███████▎                        | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 40kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert==0.4.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert==0.4.0) (1.18.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert==0.4.0) (1.5.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert==0.4.0) (1.13.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.4.0) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert==0.4.0) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert==0.4.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert==0.4.0) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert==0.4.0) (1.16.13)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert==0.4.0) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch_pretrained_bert==0.4.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch_pretrained_bert==0.4.0) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.13->boto3->pytorch_pretrained_bert==0.4.0) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.4.0\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KExDV4ctURfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = Path('/content/drive/My Drive')  # Path.cwd().parent\n",
        "DIRECTORY_TRAIN = BASE_DIR.joinpath('data', 'protechn_corpus_eval', 'train')\n",
        "DIRECTORY_DEV = BASE_DIR.joinpath('data', 'protechn_corpus_eval', 'dev')\n",
        "DIRECTORY_TEST = BASE_DIR.joinpath('data', 'protechn_corpus_eval', 'test')\n",
        "DIRECTORY_MARKUP = BASE_DIR.joinpath('data', 'protechn_corpus_eval', 'markup')\n",
        "DIRECTORY_PREDICT = BASE_DIR.joinpath('data', 'protechn_corpus_eval', 'predict')\n",
        "TECHNIQUES = [\n",
        "    'No', 'Whataboutism', 'Thought-terminating_Cliches', 'Straw_Men', 'Slogans', 'Repetition',\n",
        "    'Reductio_ad_hitlerum', 'Red_Herring', 'Obfuscation,Intentional_Vagueness,Confusion',\n",
        "    'Name_Calling,Labeling', 'Loaded_Language', 'Flag-Waving', 'Exaggeration,Minimisation',\n",
        "    'Doubt', 'Causal_Oversimplification', 'Black-and-White_Fallacy', 'Bandwagon',\n",
        "    'Appeal_to_fear-prejudice', 'Appeal_to_Authority'\n",
        "]\n",
        "HUMAN_READABLE_TECHNIQUES = [\n",
        "    \"No\", \"Whataboutism\", \"Thought-terminating Cliches\", \"Straw Men\", \"Slogans\", \"Repetition\",\n",
        "    \"Reductio ad hitlerum\", \"Red Herring\", \"Obfuscation, Intentional Vagueness, Confusion\",\n",
        "    \"Name Calling, Labeling\", \"Loaded Language\", \"Flag-Waving\", \"Exaggeration, Minimisation\",\n",
        "    \"Doubt\", \"Causal Oversimplification\", \"Black-and-White Fallacy\", \"Bandwagon\",\n",
        "    \"Appeal to fear-prejudice\", \"Appeal to Authority\"\n",
        "]\n",
        "ARTICLE = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlXfVo5CURbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_list(id_, directory=DIRECTORY_TRAIN):\n",
        "    \"\"\"\n",
        "    Функция, возвращающая список [set(), set(), ..., {Flag-Waving, Bandwagon}, ..., set(), set()].\n",
        "    \"\"\"\n",
        "\n",
        "    lines = []\n",
        "    labels_file = directory.joinpath(f'article{id_}.labels.tsv')\n",
        "    if labels_file.is_file():\n",
        "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "    with open(directory.joinpath(f'article{id_}.txt'), 'r', encoding='utf-8') as inner_f:\n",
        "        length = len(inner_f.read())\n",
        "    lst = [set() for _ in range(length)]\n",
        "    for line in lines:\n",
        "        id_, technique, left, right = line.split()\n",
        "        id_, left, right = list(map(int, (id_, left, right)))\n",
        "        for i in range(left, right):\n",
        "            lst[i].add(technique)\n",
        "    return lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghwzX1P0UYfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_of_techniques_for_id(id_, directory=DIRECTORY_TRAIN):\n",
        "    \"\"\"\n",
        "    Функция, возвращающая словарь с количеством употреблённых техник.\n",
        "    \"\"\"\n",
        "\n",
        "    lines = []\n",
        "    labels_file = directory.joinpath(f'article{id_}.labels.tsv')\n",
        "    if labels_file.is_file():\n",
        "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "    label_count_dct = defaultdict(int)\n",
        "    lst = []\n",
        "    for line in lines:\n",
        "        _, technique, left, right = line.split()\n",
        "        left, right = list(map(int, [left, right]))\n",
        "        technique = HUMAN_READABLE_TECHNIQUES[TECHNIQUES.index(technique)]\n",
        "        label_count_dct[technique] += 1\n",
        "        lst.append((technique, left, right))\n",
        "    return label_count_dct, lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KADXo44IURYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_to_text = {}\n",
        "id_to_labels = {}\n",
        "id_to_label_count = {}\n",
        "id_to_label_left_right = {}\n",
        "for directory in (DIRECTORY_TRAIN, DIRECTORY_DEV, DIRECTORY_TEST,\n",
        "                  DIRECTORY_MARKUP, DIRECTORY_PREDICT):\n",
        "    for f in directory.glob('*.txt'):\n",
        "        id_ = int(f.name.split('.')[0][ARTICLE:])\n",
        "        id_to_text[id_] = f.read_text(encoding='utf-8')\n",
        "        id_to_labels[id_] = get_list(id_, directory=directory)\n",
        "        id_to_label_count[id_], id_to_label_left_right[id_] = \\\n",
        "            get_num_of_techniques_for_id(id_, directory=directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3rNaggm4ckO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_labels(label_path):\n",
        "    \"\"\"\n",
        "    returns: [[left, right, technique, intersection, more_than_sent], ...]\n",
        "    \"\"\"\n",
        "\n",
        "    labels = []\n",
        "    if not Path(label_path).exists():\n",
        "        return labels\n",
        "    with open(label_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            _, technique, left, right = line.strip().split('\\t')\n",
        "            labels.append([int(left), int(right), technique, 0, 0])\n",
        "    labels.sort()\n",
        "    if not labels:\n",
        "        return labels\n",
        "    length = max([label[1] for label in labels])\n",
        "    visited = np.zeros(length)\n",
        "    for label in labels:\n",
        "        if sum(visited[label[0]:label[1]]):\n",
        "            label[3] = 1  # intersection\n",
        "        else:\n",
        "            visited[label[0]:label[1]] = 1\n",
        "    return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cmghb2L4c0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(articles, ids):\n",
        "    \"\"\"\n",
        "    articles: ['first text here', 'second text here', ...]\n",
        "    ids: ['id1', 'id2', ...]\n",
        "    returns: [[[id_, sentence, start, end], ...], ...]\n",
        "    \"\"\"\n",
        "\n",
        "    texts = []\n",
        "    for article, id_ in zip(articles, ids):\n",
        "        sentences = article.split('\\n')  # ['first sentence', 'second sentence', ...]\n",
        "        end = -1\n",
        "        res = []\n",
        "        for sentence in sentences:\n",
        "            start = end + 1\n",
        "            end = start + len(sentence)  # length of sequence\n",
        "            if sentence:\n",
        "                res.append([id_, sentence, start, end])  # [[id_, sentence, start, end], ...]\n",
        "        texts.append(res)\n",
        "    return texts  # [[[id_, sentence, start, end], ...], ...]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwfFqSre6Enc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(directory, is_test=False):\n",
        "    \"\"\"\n",
        "    returns: (['id1', 'id2', ...], ['text1', 'text2', ...], [[[left, right, technique, intersection, more_than_sent], ...], ...])\n",
        "    \"\"\"\n",
        "\n",
        "    ids = []\n",
        "    texts = []\n",
        "    if not is_test:\n",
        "        labels = []\n",
        "    for f in directory.glob('*.txt'):\n",
        "        ids.append(f.name.replace('article', '').replace('.txt', ''))\n",
        "        texts.append(f.read_text(encoding='utf-8'))\n",
        "        if not is_test:\n",
        "            labels.append(parse_labels(f.as_posix().replace('.txt', '.labels.tsv')))\n",
        "    if not is_test:\n",
        "        return ids, texts, labels\n",
        "    return ids, texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klQJM83z4iM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset(directory):\n",
        "    \"\"\"\n",
        "    returns: [[['id1', 'sentence1', start, end, left, right, technique, intersection, more_than_sent], ...], ...]\n",
        "    \"\"\"\n",
        "\n",
        "    ids, texts, group_labels = read_data(directory)\n",
        "    # ids: ['id1', 'id2', ...]\n",
        "    # texts: ['text1', 'text2', ...]\n",
        "    # group_labels: [[[left, right, technique, intersection, more_than_sent], ...], ...]\n",
        "    texts = clean_text(texts, ids)\n",
        "    # texts: [[['id1', 'sentence1', start, end], ['id1', 'sentence2', start, end], ...], ...]\n",
        "    res = []\n",
        "    for sents, labels in zip(texts, group_labels):\n",
        "        # sents: [['id1', 'sentence1', start, end], ['id1', 'sentence2', start, end], ...]\n",
        "        # labels: [[left, right, technique, intersection, more_than_sent], ...]\n",
        "\n",
        "        # making positive examples\n",
        "        tmp = []\n",
        "        pos_ind = [0] * len(sents)\n",
        "        for label in labels:\n",
        "            # label: [left, right, technique, intersection, more_than_sent]\n",
        "            left, right, technique, intersection, more_than_sent = label\n",
        "            for i, sent in enumerate(sents):\n",
        "                # sent: ['id1', 'sentence1', start, end]\n",
        "                *_, start, end = sent\n",
        "                if left >= start and left < end and right > end:\n",
        "                    label[4] = 1\n",
        "                    tmp.append(sent + [left, end, technique, intersection, label[4]])\n",
        "                    pos_ind[i] = 1\n",
        "                    label[0] = end + 1\n",
        "                elif left != right and left >= start and left < end and right <= end:\n",
        "                    tmp.append(sent + label)\n",
        "                    # tmp: [['id1', 'sentence1', start, end, left, right, technique, intersection, more_than_sent], ...]\n",
        "                    pos_ind[i] = 1\n",
        "\n",
        "        # making negative examples\n",
        "        dummy = [0, 0, 'O', 0, 0]\n",
        "        for i, sent in enumerate(sents):\n",
        "            if pos_ind[i] != 1:\n",
        "                tmp.append(sent + dummy)\n",
        "        res.append(tmp)\n",
        "        # res: [[['id1', 'sentence1', start, end, left, right, technique, intersection, more_than_sent], ...], ...]\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5MWgsOlTtAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_bert_dataset(dataset, is_test=False, verbose=False):\n",
        "    \"\"\"\n",
        "    dataset: [[['id1', 'sentence1', start, end, left, right, technique, intersection, more_than_sent], ...], ...]\n",
        "    returns: (\n",
        "                [ [ ['first_word', 'second_word', ...], ... ], ... ],\n",
        "                [ [ ['label1', 'label2', ...], ... ], ... ],\n",
        "                [ [ id1, id2, ... ], ... ]\n",
        "            )\n",
        "    \"\"\"\n",
        "\n",
        "    words, tags, ids= [], [], []\n",
        "    for article in dataset:\n",
        "        # article: [['id1', 'sentence1', start, end, left, right, technique, intersection, more_than_sent], ...]\n",
        "        tmp_doc, tmp_label, tmp_id = [], [], []\n",
        "        tmp_sen = article[0][1]\n",
        "        tmp_i = article[0][0]\n",
        "        label = ['O'] * len(tmp_sen.split(' '))\n",
        "        for sentence in article:\n",
        "            # sentence: ['id1', 'sentence1', start, end, left, right, technique, intersection, more_than_sent]\n",
        "            tokens = sentence[1].split(' ')\n",
        "            token_len = [len(token) for token in tokens]\n",
        "            if len(sentence) == 9:  # label exists\n",
        "                if tmp_sen != sentence[1] or (sentence[7] and is_test):\n",
        "                    tmp_label.append(label)\n",
        "                    tmp_doc.append(tmp_sen.split(' '))\n",
        "                    tmp_id.append(tmp_i)\n",
        "                    if tmp_sen != sentence[1]:\n",
        "                        label = ['O'] * len(token_len)\n",
        "                start = sentence[4] - sentence[2] \n",
        "                end = sentence[5] - sentence[2]\n",
        "                if sentence[6] != 'O':\n",
        "                    for i in range(1, len(token_len)): \n",
        "                        token_len[i] += token_len[i-1] + 1\n",
        "                    token_len[-1] += 1\n",
        "                    token_len = np.asarray(token_len)\n",
        "                    s_ind = np.min(np.where(token_len > start))\n",
        "                    tmp = np.where(token_len >= end)  \n",
        "                    if len(tmp[0]) != 0:\n",
        "                        e_ind = np.min(tmp)\n",
        "                    else: \n",
        "                        e_ind = s_ind\n",
        "                    for i in range(s_ind, e_ind+1):\n",
        "                        label[i] = sentence[6]\n",
        "                tmp_sen = sentence[1]\n",
        "                tmp_i = sentence[0]\n",
        "            else:\n",
        "                tmp_doc.append(tokens)\n",
        "                tmp_id.append(sentence[0])\n",
        "        if len(sentence) == 9:\n",
        "            tmp_label.append(label)\n",
        "            tmp_doc.append(tmp_sen.split(' '))\n",
        "            tmp_id.append(tmp_i)\n",
        "        words.append(tmp_doc) \n",
        "        tags.append(tmp_label)\n",
        "        ids.append(tmp_id)\n",
        "    if verbose:\n",
        "        print(f'words: {words}')\n",
        "        print(f'tags: {tags}')\n",
        "        print(f'ids: {ids}')\n",
        "    return words, tags, ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQUZuATA0f-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = \"training\"\n",
        "checkdir = \"checkpoints\"\n",
        "resultdir = \"results\"\n",
        "\n",
        "# either of the four variants:\n",
        "bert = False\n",
        "joint = False\n",
        "granu = False\n",
        "mgn = True\n",
        "\n",
        "assert bert or joint or granu or mgn\n",
        "assert not(bert and joint) and not(bert and granu) and not(bert and mgn) and not(joint and granu) and not(joint and mgn) and not(granu and mgn)\n",
        "\n",
        "# either of the two variants\n",
        "sigmoid_activation = False\n",
        "relu_activation = True\n",
        "assert not(sigmoid_activation and relu_activation) and (sigmoid_activation or relu_activation)\n",
        "\n",
        "trainset = './drive/My Drive/data/protechn_corpus_eval/train'\n",
        "validset = './drive/My Drive/data/protechn_corpus_eval/dev'\n",
        "testset = './drive/My Drive/data/protechn_corpus_eval/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8B5UcWW4tC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if bert:\n",
        "    num_task = 1\n",
        "    masking = 0\n",
        "    hier = 0\n",
        "elif joint:\n",
        "    num_task = 2\n",
        "    masking = 0\n",
        "    hier = 0\n",
        "elif granu:\n",
        "    num_task = 2\n",
        "    masking = 0\n",
        "    hier = 1\n",
        "elif mgn:\n",
        "    num_task = 2\n",
        "    masking = 1\n",
        "    hier = 0\n",
        "else:\n",
        "    raise ValueError(\"You should choose one of [bert, joint, granu and mgn] in options\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRQadYo-0U5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "LR = 1e-5\n",
        "ALPHA = 0.75\n",
        "N_EPOCHS = 100\n",
        "PATIENCE = 15\n",
        "INPUT_SIZE = 768\n",
        "SEQ_LEN = 212\n",
        "POS_WEIGHT = 926 / 3532"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzvrVZ8n3NL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_to_index, index_to_tag = [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF5SVAm70U_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB = [\n",
        "    (\"<PAD>\", \"O\", \"Name_Calling,Labeling\", \"Repetition\", \"Slogans\",\n",
        "     \"Appeal_to_fear-prejudice\", \"Doubt\", \"Exaggeration,Minimisation\",\n",
        "     \"Flag-Waving\", \"Loaded_Language\", \"Reductio_ad_hitlerum\", \"Bandwagon\",\n",
        "     \"Causal_Oversimplification\", \"Obfuscation,Intentional_Vagueness,Confusion\",\n",
        "     \"Appeal_to_Authority\", \"Black-and-White_Fallacy\",\n",
        "     \"Thought-terminating_Cliches\", \"Red_Herring\", \"Straw_Men\", \"Whataboutism\")\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED3Li2sO0U9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if num_task == 2:  # sentence classification\n",
        "    VOCAB.append((\"Non-prop\", \"Prop\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_6cpvx539XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(num_task):\n",
        "    tag_to_index.append({tag: idx for idx, tag in enumerate(VOCAB[i])})\n",
        "    index_to_tag.append({idx: tag for idx, tag in enumerate(VOCAB[i])})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBjTycz54G8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (New, recommended) 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/bert/vocab.txt', do_lower_case=False)  # 'bert-base-multilingual-cased', do_lower_case=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbivspLY4Nog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PropDataset(data.Dataset):\n",
        "    def __init__(self, directory_path, is_test=False, verbose=False):\n",
        "        directory = pathlib.Path(directory_path)\n",
        "        dataset = make_dataset(directory)\n",
        "        words, tags, ids = make_bert_dataset(dataset, is_test=is_test, verbose=verbose)\n",
        "        # if is_test:\n",
        "        #     words, tags, ids = make_bert_testset(dataset, verbose=verbose)\n",
        "        # else:\n",
        "        #     words, tags, ids = make_bert_dataset(dataset, verbose=verbose)\n",
        "\n",
        "        # (\n",
        "        #     [ [ ['first_word', 'second_word', ...], ... ], ... ],\n",
        "        #     [ [ ['label1', 'label2', ...], ... ], ... ],\n",
        "        #     [ [ id1, id2, ... ], ... ]\n",
        "        # )\n",
        "\n",
        "        flat_ids, flat_sents = [], []\n",
        "        tags_li = [[] for _ in range(num_task)]\n",
        "        for article_words, article_tags, article_ids in zip(words, tags, ids):\n",
        "            for inner_words, inner_tags, id_ in zip(article_words, article_tags, article_ids):\n",
        "                flat_sents.append([\"[CLS]\"] + inner_words + [\"[SEP]\"])\n",
        "                flat_ids.append(id_)\n",
        "\n",
        "                tmp_tags = []\n",
        "                if num_task == 1:  # technique classification\n",
        "                    tmp_tags.append(['O'] * len(inner_tags))\n",
        "                    for j, inner_tag in enumerate(inner_tags):\n",
        "                        if inner_tag != 'O' and inner_tag in VOCAB[0]:\n",
        "                            tmp_tags[0][j] = inner_tag\n",
        "                    tags_li[0].append([\"<PAD>\"] + tmp_tags[0] + [\"<PAD>\"])\n",
        "                else:  # sentence classification\n",
        "                    tmp_tags.append(['O'] * len(inner_tags))\n",
        "                    tmp_tags.append(['Non-prop'])\n",
        "                    for j, inner_tag in enumerate(inner_tags):\n",
        "                        if inner_tag != 'O' and inner_tag in VOCAB[0]:\n",
        "                            tmp_tags[0][j] = inner_tag\n",
        "                            tmp_tags[1] = ['Prop']\n",
        "                    for i in range(num_task):\n",
        "                        tags_li[i].append([\"<PAD>\"] + tmp_tags[i] + [\"<PAD>\"])\n",
        "\n",
        "        self.sents, self.ids, self.tags_li = flat_sents, flat_ids, tags_li\n",
        "        assert len(self.sents) == len(self.ids) == len(self.tags_li[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        words = self.sents[index]\n",
        "        id_ = self.ids[index]\n",
        "        tags = list(list(zip(*self.tags_li))[index])  # [ ['label1', 'label2', ...] ]\n",
        "\n",
        "        x, is_heads = [], []  # list of ids\n",
        "        y = [[] for _ in range(num_task)]  # list of lists of lists\n",
        "        tt = [[] for _ in range(num_task)]  # list of lists of lists\n",
        "\n",
        "        for word, tag in zip(words, tags[0]):\n",
        "            tokens = tokenizer.tokenize(word) if word not in (\"[CLS]\", \"[SEP]\") else [word]\n",
        "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0] * (len(tokens) - 1)\n",
        "            if len(xx) < len(is_head):\n",
        "                xx = xx + [100] * (len(is_head) - len(xx))  # 100 == \"[UNK]\"\n",
        "\n",
        "            tag = [tag] + [tag] * (len(tokens) - 1)\n",
        "            y[0].extend([tag_to_index[0][each] for each in tag])\n",
        "            tt[0].extend(tag)\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "\n",
        "        if num_task == 2:\n",
        "            if tags[1][1] == 'Non-prop':\n",
        "                y[1].extend([1, 0])\n",
        "                tt[1].extend([tags[1][1]])\n",
        "            elif tags[1][1] == 'Prop':\n",
        "                y[1].extend([0, 1])\n",
        "                tt[1].extend([tags[1][1]])\n",
        "\n",
        "        seqlen = len(y[0])\n",
        "        words = \" \".join([id_] + words)\n",
        "\n",
        "        for i in range(num_task):\n",
        "            tags[i] = \" \".join(tags[i])\n",
        "\n",
        "        att_mask = [1] * seqlen\n",
        "        return words, x, is_heads, att_mask, tags, y, seqlen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5fjN0FxGeLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad(batch):\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    seqlen = f(-1)\n",
        "    maxlen = SEQ_LEN\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch]  # 0: '[PAD]'\n",
        "    x = torch.LongTensor(f(1, maxlen))\n",
        "    att_mask = f(-4, maxlen)\n",
        "\n",
        "    y = []\n",
        "    tags = []\n",
        "\n",
        "    y.append(torch.LongTensor([sample[-2][0] + [0] * (maxlen - len(sample[-2][0])) for sample in batch]))\n",
        "    for i in range(num_task):\n",
        "        tags.append([sample[-3][i] for sample in batch])\n",
        "    if num_task == 2:  # sentence classification\n",
        "        y.append(torch.LongTensor([sample[-2][1] for sample in batch]))\n",
        "\n",
        "    return words, x, is_heads, att_mask, tags, y, seqlen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LLS4i6rVVJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=PATIENCE, verbose=False, filepath='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.filepath = filepath\n",
        "\n",
        "    def __call__(self, val_loss, model, filepath):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, filepath)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, filepath)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, filepath):\n",
        "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), filepath)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-WGzbxLVVQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertMultiTaskLearning(PreTrainedBertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.ModuleList([nn.Linear(config.hidden_size, len(VOCAB[i])) for i in range(num_task)])\n",
        "        self.apply(self.init_bert_weights)\n",
        "        self.masking_gate = nn.Linear(2, 1)\n",
        "\n",
        "        if num_task == 2:\n",
        "            self.merge_classifier_1 = nn.Linear(len(VOCAB[0]) + len(VOCAB[1]), len(VOCAB[0]))\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        sequence_output, pooled_output = self.bert(\n",
        "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        if num_task == 1:\n",
        "            logits = [self.classifier[i](sequence_output) for i in range(num_task)]\n",
        "        elif num_task == 2 and masking:\n",
        "            token_level = self.classifier[0](sequence_output)\n",
        "            sen_level = self.classifier[1](pooled_output)\n",
        "\n",
        "            if sigmoid_activation:\n",
        "                gate = sigmoid(self.masking_gate(sen_level))\n",
        "            else:\n",
        "                gate = relu(self.masking_gate(sen_level))\n",
        "\n",
        "            dup_gate = gate.unsqueeze(1).repeat(1, token_level.size()[1], token_level.size()[2])\n",
        "            wei_token_level = torch.mul(dup_gate, token_level)\n",
        "\n",
        "            logits = [wei_token_level, sen_level]\n",
        "        elif num_task == 2 and hier:\n",
        "            token_level = self.classifier[0](sequence_output)\n",
        "            sen_level = self.classifier[1](pooled_output)\n",
        "            dup_sen_level = sen_level.repeat(1, token_level.size()[1])\n",
        "            dup_sen_level = dup_sen_level.view(sen_level.size()[0], -1, sen_level.size()[-1])\n",
        "            logits = [\n",
        "                self.merge_classifier_1(torch.cat((token_level, dup_sen_level), 2)),\n",
        "                self.classifier[1](pooled_output)\n",
        "            ]\n",
        "        elif num_task == 2:\n",
        "            token_level = self.classifier[0](sequence_output)\n",
        "            sen_level = self.classifier[1](pooled_output)\n",
        "            logits = [token_level, sen_level]\n",
        "        else:\n",
        "            raise ValueError(\"Incorrect combination of input arguments\")\n",
        "        y_hats = [logits[i].argmax(-1) for i in range(num_task)]\n",
        "\n",
        "        return logits, y_hats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa0AFUWWYMlP",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WiR1DdzVVXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8kax4tHY5zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72jDK0GoYPpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, binary_criterion):\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    for k, batch in enumerate(iterator):\n",
        "        words, x, is_heads, att_mask, tags, y, seqlens = batch\n",
        "        att_mask = torch.Tensor(att_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(x, attention_mask=att_mask)\n",
        "\n",
        "        loss = []\n",
        "\n",
        "        for i in range(num_task):\n",
        "            logits[i] = logits[i].view(-1, logits[i].shape[-1])  # (N*T, 2)\n",
        "        y[0] = y[0].view(-1).to(DEVICE)\n",
        "        loss.append(criterion(logits[0], y[0]))\n",
        "        if num_task == 2:    \n",
        "            y[1] = y[1].float().to(DEVICE)\n",
        "            loss.append(binary_criterion(logits[1], y[1]))\n",
        "\n",
        "        if num_task == 1:\n",
        "            joint_loss = loss[0]\n",
        "        elif num_task == 2:\n",
        "            joint_loss = ALPHA*loss[0] + (1 - ALPHA)*loss[1]\n",
        "\n",
        "        joint_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(joint_loss.item())\n",
        "\n",
        "        if k % 10 == 0:  # monitoring\n",
        "            print(\"step: {}, loss0: {}\".format(k, loss[0].item()))\n",
        "\n",
        "    train_loss = np.average(train_losses)\n",
        "\n",
        "    return train_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC_wa0ZImol0",
        "colab_type": "code",
        "outputId": "ad5f241d-7c07-4be9-f67e-8d8cbee0ca9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "sorted(tag_to_index[0].items(), key=lambda elem: elem[1])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<PAD>', 0),\n",
              " ('O', 1),\n",
              " ('Name_Calling,Labeling', 2),\n",
              " ('Repetition', 3),\n",
              " ('Slogans', 4),\n",
              " ('Appeal_to_fear-prejudice', 5),\n",
              " ('Doubt', 6),\n",
              " ('Exaggeration,Minimisation', 7),\n",
              " ('Flag-Waving', 8),\n",
              " ('Loaded_Language', 9),\n",
              " ('Reductio_ad_hitlerum', 10),\n",
              " ('Bandwagon', 11),\n",
              " ('Causal_Oversimplification', 12),\n",
              " ('Obfuscation,Intentional_Vagueness,Confusion', 13),\n",
              " ('Appeal_to_Authority', 14),\n",
              " ('Black-and-White_Fallacy', 15),\n",
              " ('Thought-terminating_Cliches', 16),\n",
              " ('Red_Herring', 17),\n",
              " ('Straw_Men', 18),\n",
              " ('Whataboutism', 19)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxICMsOEZ-LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model, iterator, f, criterion, binary_criterion, baseline_1=False):\n",
        "    \"\"\" evaluation on SLC and FLC tasks \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    valid_losses = []\n",
        "\n",
        "    Words, Is_heads = [], []\n",
        "    Tags = [[] for _ in range(num_task)]\n",
        "    Y = [[] for _ in range(num_task)]\n",
        "    Y_hats = [[] for _ in range(num_task)]\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            words, x, is_heads, att_mask, tags, y, seqlens = batch\n",
        "            att_mask = torch.Tensor(att_mask)\n",
        "            logits, y_hats = model(x, attention_mask=att_mask)  # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "            loss = []\n",
        "            for i in range(num_task):\n",
        "                logits[i] = logits[i].view(-1, logits[i].shape[-1])  # (N * T, 2)\n",
        "            y[0] = y[0].view(-1).to(DEVICE)\n",
        "            loss.append(criterion(logits[0], y[0]))\n",
        "            if num_task == 2:\n",
        "                y[1] = y[1].float().to(DEVICE)\n",
        "                loss.append(binary_criterion(logits[1], y[1]))\n",
        "\n",
        "            if num_task == 1:\n",
        "                joint_loss = loss[0]\n",
        "            elif num_task == 2:\n",
        "                joint_loss = ALPHA*loss[0] + (1-ALPHA)*loss[1]\n",
        "\n",
        "            valid_losses.append(joint_loss.item())\n",
        "            Words.extend(words)\n",
        "            Is_heads.extend(is_heads)\n",
        "\n",
        "            for i in range(num_task):\n",
        "                Tags[i].extend(tags[i])\n",
        "                Y[i].extend(y[i].cpu().numpy().tolist())\n",
        "                Y_hats[i].extend(y_hats[i].cpu().numpy().tolist())\n",
        "    valid_loss = np.average(valid_losses)\n",
        "\n",
        "    with open(f, 'w', encoding='utf-8') as fout:\n",
        "        y_hats, preds = [[[] for _ in range(num_task)] for _ in range(2)]\n",
        "        if num_task == 1:\n",
        "            for words, is_heads, tags[0], y_hats[0] in zip(Words, Is_heads, *Tags, *Y_hats):\n",
        "                y_hats[0] = [hat for head, hat in zip(is_heads, y_hats[0]) if head == 1]\n",
        "                preds[0] = [index_to_tag[0][hat] for hat in y_hats[0]]\n",
        "                fout.write(words.split()[0])\n",
        "                fout.write(\"\\n\")\n",
        "                for w, t1, p_1 in zip(words.split()[2:-1], tags[0].split()[1:-1], preds[0][1:-1]):\n",
        "                    fout.write(\"{} {} {} \\n\".format(w, t1, p_1))\n",
        "                fout.write(\"\\n\")\n",
        "        else:  # num_task == 2\n",
        "            TP, FP, FN, TN = 0, 0, 0, 0\n",
        "            for words, is_heads, tags[0], tags[1], y_hats[0], y_hats[1] in zip(Words, Is_heads, *Tags, *Y_hats):\n",
        "                y_hats[0] = [hat for head, hat in zip(is_heads, y_hats[0]) if head == 1]\n",
        "                preds[0] = [index_to_tag[0][hat] for hat in y_hats[0]]\n",
        "                preds[1] = index_to_tag[1][y_hats[1]]\n",
        "\n",
        "                if baseline_1:\n",
        "                    preds[1] = 'Prop'\n",
        "\n",
        "                if tags[1].split()[1] == 'Non-prop' and preds[1] == 'Non-prop':\n",
        "                    TN += 1\n",
        "                elif tags[1].split()[1] == 'Non-prop' and preds[1] == 'Prop':\n",
        "                    FP += 1\n",
        "                elif tags[1].split()[1] == 'Prop' and preds[1] == 'Prop':\n",
        "                    TP += 1\n",
        "                elif tags[1].split()[1] == 'Prop' and preds[1] == 'Non-prop':\n",
        "                    FN += 1\n",
        "\n",
        "                fout.write(words.split()[0] + \"\\n\")\n",
        "                for w, t1, p_1 in zip(words.split()[2:-1], tags[0].split()[1:-1], preds[0][1:-1]):\n",
        "                    fout.write(\"{} {} {} {} {}\\n\".format(w, t1, tags[1].split()[1:-1][0], p_1, preds[1]))\n",
        "                fout.write(\"\\n\")\n",
        "\n",
        "            try:\n",
        "                precision = TP / (TP + FP)\n",
        "            except ZeroDivisionError:\n",
        "                precision = 1.0\n",
        "            try:\n",
        "                recall = TP / (TP + FN)\n",
        "            except ZeroDivisionError:\n",
        "                recall = 1.0\n",
        "            try:\n",
        "                f1 = 2 * precision * recall / (precision + recall)\n",
        "            except ZeroDivisionError:\n",
        "                if precision * recall == 0:\n",
        "                    f1 = 1.0\n",
        "                else:\n",
        "                    f1 = 0.0\n",
        "            print(f\"SLC precision: {precision:.4f}\")\n",
        "            print(f\"SLC recall: {recall:.4f}\")\n",
        "            print(f\"SLC f1-score: {f1:.4f}\")\n",
        "\n",
        "    ## calc metric \n",
        "    y_true, y_pred = [], []\n",
        "    for i in range(num_task):\n",
        "        y_true.append(np.array([tag_to_index[i][line.split()[i + 1]] for line in open(f, 'r', encoding='utf-8').read().splitlines() if len(line.split()) > 1]))\n",
        "        if baseline_1:\n",
        "            if i == 0:\n",
        "                key = \"Loaded_Language\"\n",
        "            else:\n",
        "                key = \"Prop\"\n",
        "            y_pred.append(np.array([tag_to_index[i][key] for line in open(f, 'r', encoding='utf-8').read().splitlines() if len(line.split()) > 1]))\n",
        "        else:\n",
        "            y_pred.append(np.array([tag_to_index[i][line.split()[i + 1 + num_task]] for line in open(f, 'r', encoding='utf-8').read().splitlines() if len(line.split()) > 1]))\n",
        "\n",
        "    num_predicted, num_correct, num_gold = 0, 0, 0\n",
        "\n",
        "    num_predicted += len(y_pred[0][y_pred[0] > 1])\n",
        "    num_correct += (np.logical_and(y_true[0] == y_pred[0], y_true[0] > 1)).astype(np.int).sum()\n",
        "    num_gold += len(y_true[0][y_true[0] > 1])\n",
        "\n",
        "    print(f\"FLC number of predicted techniques: {num_predicted}\")\n",
        "    print(f\"FLC number of correct techniques: {num_correct}\")\n",
        "    print(f\"FLC number of gold techniques: {num_gold}\")\n",
        "\n",
        "    try:\n",
        "        precision = num_correct / num_predicted\n",
        "    except ZeroDivisionError:\n",
        "        precision = 1.0\n",
        "\n",
        "    try:\n",
        "        recall = num_correct / num_gold\n",
        "    except ZeroDivisionError:\n",
        "        recall = 1.0\n",
        "\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        if precision * recall == 0:\n",
        "            f1 = 1.0\n",
        "        else:\n",
        "            f1 = 0\n",
        "\n",
        "    # final = f + \".P%.4f_R%.4f_F1%.4f\" % (precision, recall, f1)\n",
        "    # with open(f, 'w', encoding='utf-8') as fout:  # final\n",
        "    #     result = open(f, \"r\", encoding='utf-8').read()\n",
        "    #     fout.write(\"{}\\n\".format(result))\n",
        "    #     fout.write(\"precision={:4f}\\n\".format(precision))\n",
        "    #     fout.write(\"recall={:4f}\\n\".format(recall))\n",
        "    #     fout.write(\"f1={:4f}\\n\".format(f1))\n",
        "\n",
        "    # os.remove(f)\n",
        "\n",
        "    print(f\"FLC precision: {precision:.4f}\")\n",
        "    print(f\"FLC recall: {recall:.4f}\")\n",
        "    print(f\"FLC f1-score: {f1:.4f}\")\n",
        "    return precision, recall, f1, valid_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxwys9vOcXXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clear tqdm\n",
        "\n",
        "from IPython import get_ipython\n",
        "\n",
        "def tqdm_clear(*args, **kwargs):\n",
        "    from tqdm import tqdm\n",
        "    getattr(tqdm, '_instances', {}).clear()\n",
        "\n",
        "get_ipython().events.register('post_execute', tqdm_clear)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEYCI40HynE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_overlap(line_1, line_2):\n",
        "    if line_1[2] > line_2[3] or line_1[3] < line_2[2]:\n",
        "        return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvQyzcq0wtX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_duplicates(res):\n",
        "    sorted_res = sorted(res, key=itemgetter(0, 1, 2, 3))\n",
        "    ans = []\n",
        "    skip = 0\n",
        "    for i, line_1 in enumerate(sorted_res):\n",
        "        assert line_1 == sorted_res[i]\n",
        "        for j, line_2 in enumerate(sorted_res[i + 1:]):\n",
        "            skip = 0\n",
        "            if line_1[0] != line_2[0]:\n",
        "                break\n",
        "            elif line_1[1] != line_2[1]:\n",
        "                continue\n",
        "\n",
        "            if check_overlap(line_1, line_2):\n",
        "                if line_1[2] != line_2[2] or line_1[3] != line_2[3]:\n",
        "                    sorted_res[i + j + 1][2] = min(line_1[2], line_2[2])\n",
        "                    sorted_res[i + j + 1][3] = max(line_1[3], line_2[3])\n",
        "                skip = 1\n",
        "                break\n",
        "        if skip == 0:\n",
        "            ans.append(line_1)\n",
        "    return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15mNSZe30Tra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def convert(ind, flat_texts, filename):\n",
        "    \"\"\"\n",
        "    1173236160\n",
        "    Мало O Non-prop O Prop\n",
        "    того, O Non-prop O Prop\n",
        "    что O Non-prop O Prop\n",
        "    «Аркан» O Non-prop O Prop\n",
        "    приобрел O Non-prop O Prop\n",
        "    «Ведомости» O Non-prop O Prop\n",
        "    у O Non-prop O Prop\n",
        "    кипрского O Non-prop O Prop\n",
        "    офшора O Non-prop O Prop\n",
        "    за O Non-prop O Prop\n",
        "    бóльшую O Non-prop Loaded_Language Prop\n",
        "    сумму, O Non-prop O Prop\n",
        "    чем O Non-prop O Prop\n",
        "    они O Non-prop O Prop\n",
        "    <...>\n",
        "\n",
        "    1173236160\n",
        "    <...>\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as f1:\n",
        "        output = []\n",
        "        for line in f1:\n",
        "            if len(line.split()) == 1:  # if line is id\n",
        "                id_ = line.strip()\n",
        "                continue\n",
        "            elif line != '\\n':  # In the same sentence\n",
        "                tmp = [id_] + line.strip().split()  # add id to line\n",
        "                if len(tmp) == 6:  # num_task 2\n",
        "                    tmp += [tmp[-2]]\n",
        "                else:\n",
        "                    tmp += [tmp[-(1 + ind)]]\n",
        "                output.append(tmp + [len(tmp[1])])  # add word length to line\n",
        "            else:\n",
        "                output.append('\\n')\n",
        "\n",
        "    res = []\n",
        "    aid = output[0][0]\n",
        "    sub_list = [sentence for sentence in flat_texts if sentence[0] == aid]\n",
        "    sub_dic = {re.sub('\\s+', ' ', sentence): (start, end) for _, sentence, start, end in sub_list}\n",
        "\n",
        "\n",
        "    start = 0\n",
        "    end = -1\n",
        "    sentence = \"\"\n",
        "    cur = 0\n",
        "    on = 0\n",
        "\n",
        "    tmp_ans = []\n",
        "    cur_tag = 'O'\n",
        "    prop_or_not_dict = {}\n",
        "\n",
        "    slc_task = {}\n",
        "    sent_predictions = (False, False)\n",
        "\n",
        "    for line in output:  # ['36081082999', 'вора', 'O', 'Non-prop', 'O', 'Non-prop', 'O', 4]\n",
        "        # ['1173236160', 'щедрых', 'O', 'Loaded_Language', 'Loaded_Language', 6]\n",
        "        if line != '\\n':\n",
        "            aid = line[0]\n",
        "            if int(aid) not in prop_or_not_dict:\n",
        "                prop_or_not_dict[int(aid)] = [False for _ in range(len(id_to_text[int(aid)]))]\n",
        "\n",
        "            sentence += line[1] + \" \"\n",
        "            prop_or_not_prop = line[-3] != 'Non-prop'\n",
        "            sent_predictions = (sent_predictions[0] | (line[2] != 'O'),\n",
        "                                sent_predictions[1] | (line[4] != 'O'))\n",
        "\n",
        "            if line[-2] != 'O' and line[-2] != '<PAD>':\n",
        "                if on == 0:\n",
        "                    on = 1\n",
        "                    cur_tag = line[-2]\n",
        "                    start = cur\n",
        "                    end = cur + line[-1]\n",
        "                elif line[-2] == cur_tag:\n",
        "                    end = cur + line[-1]\n",
        "                else:\n",
        "                    tmp_ans.append([aid, cur_tag, start, end])\n",
        "                    cur_tag = line[-2]\n",
        "                    start = cur\n",
        "                    end = cur + line[-1]\n",
        "            else:\n",
        "                if on:\n",
        "                    tmp_ans.append([aid, cur_tag, start, end])\n",
        "                    on = 0\n",
        "            cur += line[-1] + 1\n",
        "\n",
        "        else:\n",
        "            if on:\n",
        "                tmp_ans.append([aid, cur_tag, start, end])\n",
        "                on = 0\n",
        "\n",
        "            cur = 0\n",
        "            sub_list = [sentence for sentence in flat_texts if sentence[0] == aid]\n",
        "            sub_dic = {re.sub('\\s+', ' ', sentence): (start, end) for _, sentence, start, end in sub_list}\n",
        "\n",
        "            if sentence[:-1] != \"\":\n",
        "                s, e = sub_dic.get(sentence[:-1])\n",
        "                slc_task[(s, e, sentence[:-1])] = sent_predictions\n",
        "\n",
        "            if len(tmp_ans) and sentence[:-1] != \"\":\n",
        "                s, e = sub_dic.get(sentence[:-1])\n",
        "                if prop_or_not_prop:\n",
        "                    prop_or_not_dict[int(aid)][s:e] = [True for _ in range(s, e)]\n",
        "\n",
        "            # tmp_ans: [['1173236160', 'Loaded_Language', 41, 63]]\n",
        "            # sentence: Журнал The New Times в 2016 г. назвал ее предполагаемой старшей дочерью Путина.\n",
        "            # prop_or_not_prop: True\n",
        "            # start, end: (25180, 25259)\n",
        "            # sent_predictions: (False, True)\n",
        "\n",
        "            if len(tmp_ans) and sentence[:-1] != \"\":\n",
        "                for ans in tmp_ans:\n",
        "                    ans[2] += s\n",
        "                    ans[3] += s\n",
        "                    res.append(ans)\n",
        "            sent_predictions = (False, False)\n",
        "            sentence = \"\"\n",
        "            prop_or_not_prop = False\n",
        "            \n",
        "            tmp_ans = []\n",
        "    TP, FP, FN, TN = 0, 0, 0, 0\n",
        "    for true, pred in slc_task.values():\n",
        "        if pred == true == True:\n",
        "            TP += 1\n",
        "        elif pred == true == False:\n",
        "            TN += 1\n",
        "        elif pred != true and pred == True:\n",
        "            FP += 1\n",
        "        else:\n",
        "            FN += 1\n",
        "    try:\n",
        "        precision = TP / (TP + FP)\n",
        "    except ZeroDivisionError:\n",
        "        precision = 1.0\n",
        "    try:\n",
        "        recall = TP / (TP + FN)\n",
        "    except ZeroDivisionError:\n",
        "        recall = 1.0\n",
        "    try:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    except ZeroDivisionError:\n",
        "        if precision * recall == 0:\n",
        "            f1 = 1.0\n",
        "        else:\n",
        "            f1 = 0.0\n",
        "    print(f'SLC precision: {precision:.4f}')\n",
        "    print(f'SLC recall: {recall:.4f}')\n",
        "    print(f'SLC f1_score: {f1:.4f}')\n",
        "            \n",
        "    return res, prop_or_not_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ia2PTWQZsEV",
        "colab_type": "code",
        "outputId": "e9788384-808a-4113-8577-628e7755e745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "model = BertMultiTaskLearning.from_pretrained('/content/drive/My Drive/bert')  # 'bert-base-multilingual-cased'\n",
        "print(\"Detect \", torch.cuda.device_count(), \"GPUs!\")\n",
        "model = nn.DataParallel(model)\n",
        "model.to(DEVICE)\n",
        "\n",
        "train_dataset = PropDataset(trainset, is_test=False)\n",
        "eval_dataset = PropDataset(validset, is_test=True)\n",
        "test_dataset = PropDataset(testset, is_test=True)\n",
        "\n",
        "train_iter = data.DataLoader(\n",
        "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=1, collate_fn=pad\n",
        ")\n",
        "eval_iter = data.DataLoader(\n",
        "    dataset=eval_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=1, collate_fn=pad\n",
        ")\n",
        "test_iter = data.DataLoader(\n",
        "    dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=1, collate_fn=pad\n",
        ")\n",
        "\n",
        "warmup_proportion = 0.1\n",
        "num_train_optimization_steps = int(len(train_dataset) / BATCH_SIZE) * N_EPOCHS\n",
        "param_optimizer = list(model.named_parameters())\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(\n",
        "    optimizer_grouped_parameters, lr=LR, warmup=warmup_proportion,\n",
        "    t_total=num_train_optimization_steps\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "binary_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([POS_WEIGHT]).to(DEVICE))\n",
        "\n",
        "avg_train_losses = []\n",
        "avg_valid_losses = []\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=PATIENCE, verbose=True)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detect  1 GPUs!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNsih3c5ZW_X",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfZPKZPkcXBb",
        "colab_type": "code",
        "outputId": "b9ab0a3e-ba35-48ae-b470-ba0d00168162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print(\"=========eval at epoch={epoch}=========\")\n",
        "    if not os.path.exists('checkpoints'):\n",
        "        os.makedirs('checkpoints')\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    fname = os.path.join('checkpoints', timestr)\n",
        "    spath = os.path.join('checkpoints', timestr + \".pt\")\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, binary_criterion)\n",
        "    avg_train_losses.append(train_loss.item())\n",
        "\n",
        "    precision, recall, f1, valid_loss = eval(model, eval_iter, fname, criterion, binary_criterion)\n",
        "    avg_valid_losses.append(valid_loss.item())\n",
        "\n",
        "    epoch_len = len(str(N_EPOCHS))\n",
        "    print(\n",
        "        f'[{epoch:>{epoch_len}}/{N_EPOCHS:>{epoch_len}}]     '\n",
        "        f'train_loss: {train_loss:.5f} '\n",
        "        f'valid_loss: {valid_loss:.5f}'\n",
        "    )\n",
        "\n",
        "    early_stopping(-f1, model, spath)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 2.995732545852661\n",
            "step: 10, loss0: 2.995732307434082\n",
            "step: 20, loss0: 2.995732545852661\n",
            "step: 30, loss0: 2.995732545852661\n",
            "step: 40, loss0: 2.986283540725708\n",
            "step: 50, loss0: 2.7114439010620117\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 0\n",
            "FLC number of correct techniques: 0\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: nan\n",
            "FLC recall: 0.0000\n",
            "FLC f1-score: nan\n",
            "[  1/100]     train_loss: 2.25847 valid_loss: 1.12684\n",
            "Validation loss decreased (inf --> nan).  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:119: RuntimeWarning: invalid value encountered in long_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 1.6528111696243286\n",
            "step: 10, loss0: 1.101335048675537\n",
            "step: 20, loss0: 0.7737540602684021\n",
            "step: 30, loss0: 0.5953529477119446\n",
            "step: 40, loss0: 0.6294087767601013\n",
            "step: 50, loss0: 0.9715940952301025\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 0\n",
            "FLC number of correct techniques: 0\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: nan\n",
            "FLC recall: 0.0000\n",
            "FLC f1-score: nan\n",
            "[  2/100]     train_loss: 0.71820 valid_loss: 0.55310\n",
            "Validation loss decreased (nan --> nan).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.8830757141113281\n",
            "step: 10, loss0: 0.8371393084526062\n",
            "step: 20, loss0: 0.8986011147499084\n",
            "step: 30, loss0: 0.9577757120132446\n",
            "step: 40, loss0: 0.9176406264305115\n",
            "step: 50, loss0: 0.6308437585830688\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 0\n",
            "FLC number of correct techniques: 0\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: nan\n",
            "FLC recall: 0.0000\n",
            "FLC f1-score: nan\n",
            "[  3/100]     train_loss: 0.60080 valid_loss: 0.54689\n",
            "Validation loss decreased (nan --> nan).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.5843061804771423\n",
            "step: 10, loss0: 0.556642472743988\n",
            "step: 20, loss0: 0.6148150563240051\n",
            "step: 30, loss0: 0.4831644594669342\n",
            "step: 40, loss0: 0.6614587903022766\n",
            "step: 50, loss0: 0.5745160579681396\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 6\n",
            "FLC number of correct techniques: 3\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.5000\n",
            "FLC recall: 0.0081\n",
            "FLC f1-score: 0.0159\n",
            "[  4/100]     train_loss: 0.58470 valid_loss: 0.48174\n",
            "Validation loss decreased (nan --> -0.015915).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.7123756408691406\n",
            "step: 10, loss0: 0.5612987875938416\n",
            "step: 20, loss0: 0.2826882302761078\n",
            "step: 30, loss0: 0.49261099100112915\n",
            "step: 40, loss0: 0.6377418041229248\n",
            "step: 50, loss0: 1.1412365436553955\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 40\n",
            "FLC number of correct techniques: 18\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.4500\n",
            "FLC recall: 0.0485\n",
            "FLC f1-score: 0.0876\n",
            "[  5/100]     train_loss: 0.51138 valid_loss: 0.46764\n",
            "Validation loss decreased (-0.015915 --> -0.087591).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.48542559146881104\n",
            "step: 10, loss0: 0.700892448425293\n",
            "step: 20, loss0: 0.4834657907485962\n",
            "step: 30, loss0: 0.6452991962432861\n",
            "step: 40, loss0: 0.4098976254463196\n",
            "step: 50, loss0: 0.42147138714790344\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 34\n",
            "FLC number of correct techniques: 18\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.5294\n",
            "FLC recall: 0.0485\n",
            "FLC f1-score: 0.0889\n",
            "[  6/100]     train_loss: 0.46647 valid_loss: 0.47224\n",
            "Validation loss decreased (-0.087591 --> -0.088889).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.38996967673301697\n",
            "step: 10, loss0: 0.2042437344789505\n",
            "step: 20, loss0: 0.2803341746330261\n",
            "step: 30, loss0: 0.5333185195922852\n",
            "step: 40, loss0: 0.4668382406234741\n",
            "step: 50, loss0: 0.40667465329170227\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 79\n",
            "FLC number of correct techniques: 26\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.3291\n",
            "FLC recall: 0.0701\n",
            "FLC f1-score: 0.1156\n",
            "[  7/100]     train_loss: 0.38279 valid_loss: 0.49490\n",
            "Validation loss decreased (-0.088889 --> -0.115556).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.37577253580093384\n",
            "step: 10, loss0: 0.33436688780784607\n",
            "step: 20, loss0: 0.6897045373916626\n",
            "step: 30, loss0: 0.3305339515209198\n",
            "step: 40, loss0: 0.36742091178894043\n",
            "step: 50, loss0: 0.3120885491371155\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 164\n",
            "FLC number of correct techniques: 23\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1402\n",
            "FLC recall: 0.0620\n",
            "FLC f1-score: 0.0860\n",
            "[  8/100]     train_loss: 0.31224 valid_loss: 0.53323\n",
            "EarlyStopping counter: 1 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.3610635995864868\n",
            "step: 10, loss0: 0.29921871423721313\n",
            "step: 20, loss0: 0.3239061236381531\n",
            "step: 30, loss0: 0.18737605214118958\n",
            "step: 40, loss0: 0.1508331000804901\n",
            "step: 50, loss0: 0.21006783843040466\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 295\n",
            "FLC number of correct techniques: 32\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1085\n",
            "FLC recall: 0.0863\n",
            "FLC f1-score: 0.0961\n",
            "[  9/100]     train_loss: 0.25426 valid_loss: 0.52942\n",
            "EarlyStopping counter: 2 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.23839032649993896\n",
            "step: 10, loss0: 0.11864680051803589\n",
            "step: 20, loss0: 0.2013516128063202\n",
            "step: 30, loss0: 0.18797428905963898\n",
            "step: 40, loss0: 0.4097566604614258\n",
            "step: 50, loss0: 0.257759690284729\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 146\n",
            "FLC number of correct techniques: 28\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1918\n",
            "FLC recall: 0.0755\n",
            "FLC f1-score: 0.1083\n",
            "[ 10/100]     train_loss: 0.20538 valid_loss: 0.58558\n",
            "EarlyStopping counter: 3 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.12912382185459137\n",
            "step: 10, loss0: 0.23829904198646545\n",
            "step: 20, loss0: 0.11944837123155594\n",
            "step: 30, loss0: 0.13629703223705292\n",
            "step: 40, loss0: 0.12441310286521912\n",
            "step: 50, loss0: 0.3624827563762665\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 224\n",
            "FLC number of correct techniques: 31\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1384\n",
            "FLC recall: 0.0836\n",
            "FLC f1-score: 0.1042\n",
            "[ 11/100]     train_loss: 0.17705 valid_loss: 0.57279\n",
            "EarlyStopping counter: 4 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.13773851096630096\n",
            "step: 10, loss0: 0.061528678983449936\n",
            "step: 20, loss0: 0.07438313215970993\n",
            "step: 30, loss0: 0.11228550970554352\n",
            "step: 40, loss0: 0.24351957440376282\n",
            "step: 50, loss0: 0.031636398285627365\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 171\n",
            "FLC number of correct techniques: 27\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1579\n",
            "FLC recall: 0.0728\n",
            "FLC f1-score: 0.0996\n",
            "[ 12/100]     train_loss: 0.15284 valid_loss: 0.60206\n",
            "EarlyStopping counter: 5 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.03136267140507698\n",
            "step: 10, loss0: 0.0772215947508812\n",
            "step: 20, loss0: 0.06943865865468979\n",
            "step: 30, loss0: 0.058708012104034424\n",
            "step: 40, loss0: 0.0928974598646164\n",
            "step: 50, loss0: 0.08145934343338013\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 224\n",
            "FLC number of correct techniques: 29\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1295\n",
            "FLC recall: 0.0782\n",
            "FLC f1-score: 0.0975\n",
            "[ 13/100]     train_loss: 0.13590 valid_loss: 0.62518\n",
            "EarlyStopping counter: 6 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.06671386957168579\n",
            "step: 10, loss0: 0.06824520975351334\n",
            "step: 20, loss0: 0.057332295924425125\n",
            "step: 30, loss0: 0.07045809179544449\n",
            "step: 40, loss0: 0.05828719958662987\n",
            "step: 50, loss0: 0.07342061400413513\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 129\n",
            "FLC number of correct techniques: 26\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2016\n",
            "FLC recall: 0.0701\n",
            "FLC f1-score: 0.1040\n",
            "[ 14/100]     train_loss: 0.12367 valid_loss: 0.68808\n",
            "EarlyStopping counter: 7 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.097984679043293\n",
            "step: 10, loss0: 0.019676243886351585\n",
            "step: 20, loss0: 0.02782271057367325\n",
            "step: 30, loss0: 0.06290530413389206\n",
            "step: 40, loss0: 0.03322357311844826\n",
            "step: 50, loss0: 0.05380665510892868\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 253\n",
            "FLC number of correct techniques: 31\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1225\n",
            "FLC recall: 0.0836\n",
            "FLC f1-score: 0.0994\n",
            "[ 15/100]     train_loss: 0.11577 valid_loss: 0.65193\n",
            "EarlyStopping counter: 8 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.05869249999523163\n",
            "step: 10, loss0: 0.03260551765561104\n",
            "step: 20, loss0: 0.053957562893629074\n",
            "step: 30, loss0: 0.035847634077072144\n",
            "step: 40, loss0: 0.040194448083639145\n",
            "step: 50, loss0: 0.05494731292128563\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 192\n",
            "FLC number of correct techniques: 31\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1615\n",
            "FLC recall: 0.0836\n",
            "FLC f1-score: 0.1101\n",
            "[ 16/100]     train_loss: 0.10908 valid_loss: 0.69588\n",
            "EarlyStopping counter: 9 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.012009058147668839\n",
            "step: 10, loss0: 0.028890276327729225\n",
            "step: 20, loss0: 0.027169490233063698\n",
            "step: 30, loss0: 0.05922819674015045\n",
            "step: 40, loss0: 0.04319514334201813\n",
            "step: 50, loss0: 0.031047649681568146\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 134\n",
            "FLC number of correct techniques: 24\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1791\n",
            "FLC recall: 0.0647\n",
            "FLC f1-score: 0.0950\n",
            "[ 17/100]     train_loss: 0.10597 valid_loss: 0.73394\n",
            "EarlyStopping counter: 10 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.03604575991630554\n",
            "step: 10, loss0: 0.1083662211894989\n",
            "step: 20, loss0: 0.02329583838582039\n",
            "step: 30, loss0: 0.03050190396606922\n",
            "step: 40, loss0: 0.03532702475786209\n",
            "step: 50, loss0: 0.049017664045095444\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 216\n",
            "FLC number of correct techniques: 28\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1296\n",
            "FLC recall: 0.0755\n",
            "FLC f1-score: 0.0954\n",
            "[ 18/100]     train_loss: 0.10164 valid_loss: 0.70574\n",
            "EarlyStopping counter: 11 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.08208213746547699\n",
            "step: 10, loss0: 0.04962393268942833\n",
            "step: 20, loss0: 0.06018828600645065\n",
            "step: 30, loss0: 0.03551387041807175\n",
            "step: 40, loss0: 0.034823331981897354\n",
            "step: 50, loss0: 0.023112056776881218\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 114\n",
            "FLC number of correct techniques: 23\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2018\n",
            "FLC recall: 0.0620\n",
            "FLC f1-score: 0.0948\n",
            "[ 19/100]     train_loss: 0.09512 valid_loss: 0.75720\n",
            "EarlyStopping counter: 12 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.022799892351031303\n",
            "step: 10, loss0: 0.03914135694503784\n",
            "step: 20, loss0: 0.09752584993839264\n",
            "step: 30, loss0: 0.019231567159295082\n",
            "step: 40, loss0: 0.05954287573695183\n",
            "step: 50, loss0: 0.049189284443855286\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 175\n",
            "FLC number of correct techniques: 32\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1829\n",
            "FLC recall: 0.0863\n",
            "FLC f1-score: 0.1172\n",
            "[ 20/100]     train_loss: 0.09055 valid_loss: 0.76538\n",
            "Validation loss decreased (-0.115556 --> -0.117216).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.025591200217604637\n",
            "step: 10, loss0: 0.01052323542535305\n",
            "step: 20, loss0: 0.015080014243721962\n",
            "step: 30, loss0: 0.018101144582033157\n",
            "step: 40, loss0: 0.024732835590839386\n",
            "step: 50, loss0: 0.025791581720113754\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 163\n",
            "FLC number of correct techniques: 31\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1902\n",
            "FLC recall: 0.0836\n",
            "FLC f1-score: 0.1161\n",
            "[ 21/100]     train_loss: 0.08606 valid_loss: 0.75644\n",
            "EarlyStopping counter: 1 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.015646126121282578\n",
            "step: 10, loss0: 0.010331765748560429\n",
            "step: 20, loss0: 0.01947411522269249\n",
            "step: 30, loss0: 0.04154087230563164\n",
            "step: 40, loss0: 0.018836382776498795\n",
            "step: 50, loss0: 0.010391117073595524\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 212\n",
            "FLC number of correct techniques: 34\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1604\n",
            "FLC recall: 0.0916\n",
            "FLC f1-score: 0.1166\n",
            "[ 22/100]     train_loss: 0.09092 valid_loss: 0.75815\n",
            "EarlyStopping counter: 2 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.05379077419638634\n",
            "step: 10, loss0: 0.04809652268886566\n",
            "step: 20, loss0: 0.05212082713842392\n",
            "step: 30, loss0: 0.015219252556562424\n",
            "step: 40, loss0: 0.014789346605539322\n",
            "step: 50, loss0: 0.03217209130525589\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 230\n",
            "FLC number of correct techniques: 32\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1391\n",
            "FLC recall: 0.0863\n",
            "FLC f1-score: 0.1065\n",
            "[ 23/100]     train_loss: 0.08381 valid_loss: 0.75518\n",
            "EarlyStopping counter: 3 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.013824994675815105\n",
            "step: 10, loss0: 0.020100075751543045\n",
            "step: 20, loss0: 0.01715828664600849\n",
            "step: 30, loss0: 0.05152606591582298\n",
            "step: 40, loss0: 0.027102511376142502\n",
            "step: 50, loss0: 0.006325139664113522\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 138\n",
            "FLC number of correct techniques: 28\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2029\n",
            "FLC recall: 0.0755\n",
            "FLC f1-score: 0.1100\n",
            "[ 24/100]     train_loss: 0.07680 valid_loss: 0.81627\n",
            "EarlyStopping counter: 4 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.025750603526830673\n",
            "step: 10, loss0: 0.008373888209462166\n",
            "step: 20, loss0: 0.011881355196237564\n",
            "step: 30, loss0: 0.013737525790929794\n",
            "step: 40, loss0: 0.025805994868278503\n",
            "step: 50, loss0: 0.014971512369811535\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 201\n",
            "FLC number of correct techniques: 30\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1493\n",
            "FLC recall: 0.0809\n",
            "FLC f1-score: 0.1049\n",
            "[ 25/100]     train_loss: 0.07325 valid_loss: 0.81947\n",
            "EarlyStopping counter: 5 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.012112309224903584\n",
            "step: 10, loss0: 0.003199578495696187\n",
            "step: 20, loss0: 0.013089003972709179\n",
            "step: 30, loss0: 0.017060577869415283\n",
            "step: 40, loss0: 0.02212207205593586\n",
            "step: 50, loss0: 0.009386173449456692\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 144\n",
            "FLC number of correct techniques: 26\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1806\n",
            "FLC recall: 0.0701\n",
            "FLC f1-score: 0.1010\n",
            "[ 26/100]     train_loss: 0.07114 valid_loss: 0.84560\n",
            "EarlyStopping counter: 6 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.004741523880511522\n",
            "step: 10, loss0: 0.008226318284869194\n",
            "step: 20, loss0: 0.009133248589932919\n",
            "step: 30, loss0: 0.018807273358106613\n",
            "step: 40, loss0: 0.00634533679112792\n",
            "step: 50, loss0: 0.034391362220048904\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 179\n",
            "FLC number of correct techniques: 33\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1844\n",
            "FLC recall: 0.0889\n",
            "FLC f1-score: 0.1200\n",
            "[ 27/100]     train_loss: 0.06759 valid_loss: 0.89794\n",
            "Validation loss decreased (-0.117216 --> -0.120000).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.005218584090471268\n",
            "step: 10, loss0: 0.03248105198144913\n",
            "step: 20, loss0: 0.0380697026848793\n",
            "step: 30, loss0: 0.029151370748877525\n",
            "step: 40, loss0: 0.027455247938632965\n",
            "step: 50, loss0: 0.02810417115688324\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 282\n",
            "FLC number of correct techniques: 36\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1277\n",
            "FLC recall: 0.0970\n",
            "FLC f1-score: 0.1103\n",
            "[ 28/100]     train_loss: 0.06644 valid_loss: 0.82099\n",
            "EarlyStopping counter: 1 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.04334649071097374\n",
            "step: 10, loss0: 0.004442430566996336\n",
            "step: 20, loss0: 0.02469240128993988\n",
            "step: 30, loss0: 0.012742848135530949\n",
            "step: 40, loss0: 0.00796660128980875\n",
            "step: 50, loss0: 0.009283197112381458\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 175\n",
            "FLC number of correct techniques: 35\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2000\n",
            "FLC recall: 0.0943\n",
            "FLC f1-score: 0.1282\n",
            "[ 29/100]     train_loss: 0.06676 valid_loss: 0.94967\n",
            "Validation loss decreased (-0.120000 --> -0.128205).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.019524268805980682\n",
            "step: 10, loss0: 0.03683935105800629\n",
            "step: 20, loss0: 0.004311234690248966\n",
            "step: 30, loss0: 0.007576088886708021\n",
            "step: 40, loss0: 0.061855606734752655\n",
            "step: 50, loss0: 0.021142765879631042\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 139\n",
            "FLC number of correct techniques: 26\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1871\n",
            "FLC recall: 0.0701\n",
            "FLC f1-score: 0.1020\n",
            "[ 30/100]     train_loss: 0.06016 valid_loss: 1.00540\n",
            "EarlyStopping counter: 1 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.008275017142295837\n",
            "step: 10, loss0: 0.029191341251134872\n",
            "step: 20, loss0: 0.007244779262691736\n",
            "step: 30, loss0: 0.008655411191284657\n",
            "step: 40, loss0: 0.019148852676153183\n",
            "step: 50, loss0: 0.014738351106643677\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 161\n",
            "FLC number of correct techniques: 30\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1863\n",
            "FLC recall: 0.0809\n",
            "FLC f1-score: 0.1128\n",
            "[ 31/100]     train_loss: 0.05757 valid_loss: 1.00820\n",
            "EarlyStopping counter: 2 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.013320096768438816\n",
            "step: 10, loss0: 0.007990374229848385\n",
            "step: 20, loss0: 0.013000251725316048\n",
            "step: 30, loss0: 0.014184271916747093\n",
            "step: 40, loss0: 0.010964715853333473\n",
            "step: 50, loss0: 0.0175043772906065\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 248\n",
            "FLC number of correct techniques: 36\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1452\n",
            "FLC recall: 0.0970\n",
            "FLC f1-score: 0.1163\n",
            "[ 32/100]     train_loss: 0.05576 valid_loss: 0.94564\n",
            "EarlyStopping counter: 3 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.003033190034329891\n",
            "step: 10, loss0: 0.03183043375611305\n",
            "step: 20, loss0: 0.016947869211435318\n",
            "step: 30, loss0: 0.011378325521945953\n",
            "step: 40, loss0: 0.007082618307322264\n",
            "step: 50, loss0: 0.018205467611551285\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 195\n",
            "FLC number of correct techniques: 34\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1744\n",
            "FLC recall: 0.0916\n",
            "FLC f1-score: 0.1201\n",
            "[ 33/100]     train_loss: 0.05417 valid_loss: 1.04631\n",
            "EarlyStopping counter: 4 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.009820993058383465\n",
            "step: 10, loss0: 0.0039834920316934586\n",
            "step: 20, loss0: 0.057325031608343124\n",
            "step: 30, loss0: 0.0009307699510827661\n",
            "step: 40, loss0: 0.007575205992907286\n",
            "step: 50, loss0: 0.010734567418694496\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 220\n",
            "FLC number of correct techniques: 35\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1591\n",
            "FLC recall: 0.0943\n",
            "FLC f1-score: 0.1184\n",
            "[ 34/100]     train_loss: 0.05373 valid_loss: 1.07360\n",
            "EarlyStopping counter: 5 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.013056883588433266\n",
            "step: 10, loss0: 0.01966814696788788\n",
            "step: 20, loss0: 0.008624574169516563\n",
            "step: 30, loss0: 0.015386048704385757\n",
            "step: 40, loss0: 0.002717761555686593\n",
            "step: 50, loss0: 0.00349462334997952\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 166\n",
            "FLC number of correct techniques: 28\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1687\n",
            "FLC recall: 0.0755\n",
            "FLC f1-score: 0.1043\n",
            "[ 35/100]     train_loss: 0.05279 valid_loss: 1.13807\n",
            "EarlyStopping counter: 6 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.008728149347007275\n",
            "step: 10, loss0: 0.003670753911137581\n",
            "step: 20, loss0: 0.024807244539260864\n",
            "step: 30, loss0: 0.006724586710333824\n",
            "step: 40, loss0: 0.013462821952998638\n",
            "step: 50, loss0: 0.004614019300788641\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 142\n",
            "FLC number of correct techniques: 32\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2254\n",
            "FLC recall: 0.0863\n",
            "FLC f1-score: 0.1248\n",
            "[ 36/100]     train_loss: 0.05405 valid_loss: 1.22644\n",
            "EarlyStopping counter: 7 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.01887911558151245\n",
            "step: 10, loss0: 0.011314944364130497\n",
            "step: 20, loss0: 0.010164654813706875\n",
            "step: 30, loss0: 0.015798326581716537\n",
            "step: 40, loss0: 0.013111060485243797\n",
            "step: 50, loss0: 0.012944856658577919\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 170\n",
            "FLC number of correct techniques: 31\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1824\n",
            "FLC recall: 0.0836\n",
            "FLC f1-score: 0.1146\n",
            "[ 37/100]     train_loss: 0.05277 valid_loss: 1.16350\n",
            "EarlyStopping counter: 8 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.016109980642795563\n",
            "step: 10, loss0: 0.0036709990818053484\n",
            "step: 20, loss0: 0.006671193987131119\n",
            "step: 30, loss0: 0.03194640949368477\n",
            "step: 40, loss0: 0.012031844817101955\n",
            "step: 50, loss0: 0.01034783199429512\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 170\n",
            "FLC number of correct techniques: 33\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1941\n",
            "FLC recall: 0.0889\n",
            "FLC f1-score: 0.1220\n",
            "[ 38/100]     train_loss: 0.04999 valid_loss: 1.15125\n",
            "EarlyStopping counter: 9 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.010272516869008541\n",
            "step: 10, loss0: 0.004128953441977501\n",
            "step: 20, loss0: 0.0037652652245014906\n",
            "step: 30, loss0: 0.003505403408780694\n",
            "step: 40, loss0: 0.00792117603123188\n",
            "step: 50, loss0: 0.011775273829698563\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 160\n",
            "FLC number of correct techniques: 33\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2062\n",
            "FLC recall: 0.0889\n",
            "FLC f1-score: 0.1243\n",
            "[ 39/100]     train_loss: 0.05200 valid_loss: 1.23650\n",
            "EarlyStopping counter: 10 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.005512115079909563\n",
            "step: 10, loss0: 0.002896602964028716\n",
            "step: 20, loss0: 0.009529052302241325\n",
            "step: 30, loss0: 0.004167933017015457\n",
            "step: 40, loss0: 0.006266485899686813\n",
            "step: 50, loss0: 0.016281764954328537\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 127\n",
            "FLC number of correct techniques: 27\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2126\n",
            "FLC recall: 0.0728\n",
            "FLC f1-score: 0.1084\n",
            "[ 40/100]     train_loss: 0.04912 valid_loss: 1.38648\n",
            "EarlyStopping counter: 11 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.003496588906273246\n",
            "step: 10, loss0: 0.0021739681251347065\n",
            "step: 20, loss0: 0.012459063902497292\n",
            "step: 30, loss0: 0.0099564203992486\n",
            "step: 40, loss0: 0.007114921230822802\n",
            "step: 50, loss0: 0.013435252010822296\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 165\n",
            "FLC number of correct techniques: 33\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2000\n",
            "FLC recall: 0.0889\n",
            "FLC f1-score: 0.1231\n",
            "[ 41/100]     train_loss: 0.05071 valid_loss: 1.27845\n",
            "EarlyStopping counter: 12 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.005156808067113161\n",
            "step: 10, loss0: 0.0026014638133347034\n",
            "step: 20, loss0: 0.014852669090032578\n",
            "step: 30, loss0: 0.00775077985599637\n",
            "step: 40, loss0: 0.0034241972025483847\n",
            "step: 50, loss0: 0.013792217709124088\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 159\n",
            "FLC number of correct techniques: 32\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.2013\n",
            "FLC recall: 0.0863\n",
            "FLC f1-score: 0.1208\n",
            "[ 42/100]     train_loss: 0.04938 valid_loss: 1.30273\n",
            "EarlyStopping counter: 13 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.012582902796566486\n",
            "step: 10, loss0: 0.005776974372565746\n",
            "step: 20, loss0: 0.011865003034472466\n",
            "step: 30, loss0: 0.11391933262348175\n",
            "step: 40, loss0: 0.009194961749017239\n",
            "step: 50, loss0: 0.03175834193825722\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 199\n",
            "FLC number of correct techniques: 39\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1960\n",
            "FLC recall: 0.1051\n",
            "FLC f1-score: 0.1368\n",
            "[ 43/100]     train_loss: 0.04906 valid_loss: 1.22544\n",
            "Validation loss decreased (-0.128205 --> -0.136842).  Saving model ...\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.01184502150863409\n",
            "step: 10, loss0: 0.0034808609634637833\n",
            "step: 20, loss0: 0.014371140860021114\n",
            "step: 30, loss0: 0.0007259533158503473\n",
            "step: 40, loss0: 0.008119762875139713\n",
            "step: 50, loss0: 0.0015567219816148281\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 183\n",
            "FLC number of correct techniques: 35\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1913\n",
            "FLC recall: 0.0943\n",
            "FLC f1-score: 0.1264\n",
            "[ 44/100]     train_loss: 0.04687 valid_loss: 1.27412\n",
            "EarlyStopping counter: 1 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.008221899159252644\n",
            "step: 10, loss0: 0.004665034357458353\n",
            "step: 20, loss0: 0.0056370459496974945\n",
            "step: 30, loss0: 0.0055877165868878365\n",
            "step: 40, loss0: 0.002188005018979311\n",
            "step: 50, loss0: 0.006990390829741955\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 171\n",
            "FLC number of correct techniques: 32\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1871\n",
            "FLC recall: 0.0863\n",
            "FLC f1-score: 0.1181\n",
            "[ 45/100]     train_loss: 0.04618 valid_loss: 1.30236\n",
            "EarlyStopping counter: 2 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.00405515544116497\n",
            "step: 10, loss0: 0.00910719484090805\n",
            "step: 20, loss0: 0.008633457124233246\n",
            "step: 30, loss0: 0.027241859585046768\n",
            "step: 40, loss0: 0.014089978300035\n",
            "step: 50, loss0: 0.010706235654652119\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 210\n",
            "FLC number of correct techniques: 35\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1667\n",
            "FLC recall: 0.0943\n",
            "FLC f1-score: 0.1205\n",
            "[ 46/100]     train_loss: 0.04568 valid_loss: 1.26234\n",
            "EarlyStopping counter: 3 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.004146113060414791\n",
            "step: 10, loss0: 0.0025472519919276237\n",
            "step: 20, loss0: 0.0031436614226549864\n",
            "step: 30, loss0: 0.00507366843521595\n",
            "step: 40, loss0: 0.009748385287821293\n",
            "step: 50, loss0: 0.003587826155126095\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 154\n",
            "FLC number of correct techniques: 29\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1883\n",
            "FLC recall: 0.0782\n",
            "FLC f1-score: 0.1105\n",
            "[ 47/100]     train_loss: 0.04563 valid_loss: 1.40547\n",
            "EarlyStopping counter: 4 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.00674383994191885\n",
            "step: 10, loss0: 0.004403811413794756\n",
            "step: 20, loss0: 0.005233961157500744\n",
            "step: 30, loss0: 0.012384985573589802\n",
            "step: 40, loss0: 0.004408996552228928\n",
            "step: 50, loss0: 0.003420343156903982\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 156\n",
            "FLC number of correct techniques: 30\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1923\n",
            "FLC recall: 0.0809\n",
            "FLC f1-score: 0.1139\n",
            "[ 48/100]     train_loss: 0.04507 valid_loss: 1.46519\n",
            "EarlyStopping counter: 5 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.0035072858445346355\n",
            "step: 10, loss0: 0.01679118163883686\n",
            "step: 20, loss0: 0.008895072154700756\n",
            "step: 30, loss0: 0.010206903330981731\n",
            "step: 40, loss0: 0.01094652060419321\n",
            "step: 50, loss0: 0.004004604648798704\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 165\n",
            "FLC number of correct techniques: 30\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1818\n",
            "FLC recall: 0.0809\n",
            "FLC f1-score: 0.1119\n",
            "[ 49/100]     train_loss: 0.04640 valid_loss: 1.42500\n",
            "EarlyStopping counter: 6 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.008555594831705093\n",
            "step: 10, loss0: 0.00791714247316122\n",
            "step: 20, loss0: 0.0028214917983859777\n",
            "step: 30, loss0: 0.004082570318132639\n",
            "step: 40, loss0: 0.012604689225554466\n",
            "step: 50, loss0: 0.016492793336510658\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 190\n",
            "FLC number of correct techniques: 29\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1526\n",
            "FLC recall: 0.0782\n",
            "FLC f1-score: 0.1034\n",
            "[ 50/100]     train_loss: 0.04696 valid_loss: 1.37934\n",
            "EarlyStopping counter: 7 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.003035704605281353\n",
            "step: 10, loss0: 0.004341738298535347\n",
            "step: 20, loss0: 0.0050797415897250175\n",
            "step: 30, loss0: 0.0035782181657850742\n",
            "step: 40, loss0: 0.022971028462052345\n",
            "step: 50, loss0: 0.007268683519214392\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 163\n",
            "FLC number of correct techniques: 29\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1779\n",
            "FLC recall: 0.0782\n",
            "FLC f1-score: 0.1086\n",
            "[ 51/100]     train_loss: 0.04436 valid_loss: 1.48727\n",
            "EarlyStopping counter: 8 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.00404806062579155\n",
            "step: 10, loss0: 0.005928413011133671\n",
            "step: 20, loss0: 0.006173454225063324\n",
            "step: 30, loss0: 0.002659318968653679\n",
            "step: 40, loss0: 0.007466800976544619\n",
            "step: 50, loss0: 0.03240586444735527\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 243\n",
            "FLC number of correct techniques: 37\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1523\n",
            "FLC recall: 0.0997\n",
            "FLC f1-score: 0.1205\n",
            "[ 52/100]     train_loss: 0.04629 valid_loss: 1.30329\n",
            "EarlyStopping counter: 9 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.006396708078682423\n",
            "step: 10, loss0: 0.004254219122231007\n",
            "step: 20, loss0: 0.004566715098917484\n",
            "step: 30, loss0: 0.004048457369208336\n",
            "step: 40, loss0: 0.008081006817519665\n",
            "step: 50, loss0: 0.020619455724954605\n",
            "SLC precision: 1.0000\n",
            "SLC recall: 0.0000\n",
            "SLC f1-score: 0.0000\n",
            "FLC number of predicted techniques: 171\n",
            "FLC number of correct techniques: 32\n",
            "FLC number of gold techniques: 371\n",
            "FLC precision: 0.1871\n",
            "FLC recall: 0.0863\n",
            "FLC f1-score: 0.1181\n",
            "[ 53/100]     train_loss: 0.04362 valid_loss: 1.46569\n",
            "EarlyStopping counter: 10 out of 15\n",
            "=========eval at epoch={epoch}=========\n",
            "step: 0, loss0: 0.002758772810921073\n",
            "step: 10, loss0: 0.004606903996318579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-d-pn7Y7Xw",
        "colab_type": "code",
        "outputId": "6165f9bf-e1e0-445d-c5f4-8877d15ffb8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pt_files = list(Path('checkpoints').glob('*.pt'))\n",
        "if pt_files:\n",
        "    file_path = sorted(pt_files)[-1]\n",
        "    print(f'last model: {file_path.as_posix()}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last model: checkpoints/20200529-204137.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO26zXQdg-K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv checkpoints/20200529-204137.pt \"drive/My Drive/data/protechn_corpus_eval/BERT_MULTIGRAN_model_sigmoid_ru.pt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dH4MEgaZddn",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_eQ3OHeYr8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model_type):\n",
        "    \"\"\" load the the model \"\"\"\n",
        "    model = BertMultiTaskLearning.from_pretrained('/content/drive/My Drive/bert')  # 'bert-base-multilingual-cased'\n",
        "    print(\"Detect \", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    spath = f\"drive/My Drive/data/protechn_corpus_eval/{model_type}.pt\"\n",
        "    model.load_state_dict(torch.load(spath))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNbLxcDRKDRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtN-QaiaZQOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = os.path.join('results', 'tmp.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwsniFPloVh6",
        "colab_type": "code",
        "outputId": "c2213806-664b-495e-8d9c-facc43a37033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# # Baseline quality\n",
        "# num_task = 2\n",
        "# masking = 1\n",
        "# hier = 0\n",
        "\n",
        "# model = load_model('BERT_MULTIGRAN_model_relu_ru')\n",
        "# eval(model, test_iter, res, criterion, binary_criterion, baseline_1=True);"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detect  1 GPUs!\n",
            "SLC precision: 0.2884\n",
            "SLC recall: 1.0000\n",
            "SLC f1-score: 0.4477\n",
            "FLC number of predicted techniques: 16605\n",
            "FLC number of correct techniques: 327\n",
            "FLC number of gold techniques: 2018\n",
            "FLC precision: 0.0197\n",
            "FLC recall: 0.1620\n",
            "FLC f1-score: 0.0351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLaKyY6gq9PG",
        "colab_type": "code",
        "outputId": "ffb741e3-ddec-4fad-a85c-d4100fdbd074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = load_model('BERT_MULTIGRAN_model_relu_ru')  # BERT_JOINT_model_ru"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detect  1 GPUs!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqFabQKOrXID",
        "colab_type": "code",
        "outputId": "20db2765-47c2-42c0-f344-99ed791677ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "eval(model, test_iter, res, criterion, binary_criterion);"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SLC precision: 0.6462\n",
            "SLC recall: 0.6304\n",
            "SLC f1-score: 0.6382\n",
            "FLC number of predicted techniques: 645\n",
            "FLC number of correct techniques: 214\n",
            "FLC number of gold techniques: 2018\n",
            "FLC precision: 0.3318\n",
            "FLC recall: 0.1060\n",
            "FLC f1-score: 0.1607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S4TB05i0UxA",
        "colab_type": "code",
        "outputId": "5b9065e7-9d5e-4527-a1fc-0de1b6478a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "directory = pathlib.Path('./drive/My Drive/data/protechn_corpus_eval/test')\n",
        "ids, texts = read_data(directory, is_test=True)\n",
        "\n",
        "t_texts = clean_text(texts, ids)\n",
        "flat_texts = [sentence for article in t_texts for sentence in article]\n",
        "\n",
        "fi, prop_sents = convert(num_task - 1, flat_texts, res)\n",
        "results = remove_duplicates(fi)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SLC precision: 0.6508\n",
            "SLC recall: 0.3994\n",
            "SLC f1_score: 0.4950\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}